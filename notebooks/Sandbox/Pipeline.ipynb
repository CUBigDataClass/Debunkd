{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import configparser\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../gnip_auth.ini')\n",
    "api_user = config['GNIP_AUTH']['username']\n",
    "api_pass = config['GNIP_AUTH']['password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Using GNIP API search filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GnipData():\n",
    "    \n",
    "    def __init__(maxResults, geolocation, fromDate, toDate):\n",
    "        self.maxResults = maxResults\n",
    "        self.geolocation = geolocation\n",
    "        self.fromDate = fromDate\n",
    "        self.toDate = toDate\n",
    "        self._cred = (api_user, api_pass)\n",
    "        self.url = \"https://gnip-api.twitter.com/search/\" \\\n",
    "        \"fullarchive/accounts/greg-students/prod.json\"\n",
    "    \n",
    "    \n",
    "    def fetchTweets(queries):\n",
    "        params = {'fromDate':'201401010000',\n",
    "                  'toDate': '201703200000',\n",
    "                  'maxResults':self.maxResults}\n",
    "        \n",
    "        for query in queries:\n",
    "            params['query'] = query\n",
    "            \n",
    "            response = requests.get(self.url,\n",
    "                                    params=params,\n",
    "                                    auth=self._cred)\n",
    "\n",
    "            queueKafka(response.json()['results'])\n",
    "            \n",
    "            while ('next' in response.json().keys()):\n",
    "                params['next'] = response.json()['next']\n",
    "                \n",
    "                response = requests.get(url,\n",
    "                                        params=params,\n",
    "                                        auth=self._cred)\n",
    "\n",
    "                queueKafka(response.json()['results'])\n",
    "\n",
    "    \n",
    "    def queueKafka(json_data):\n",
    "        # queue json data in kafka\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SparkProcessing():\n",
    "    \n",
    "    def __init__(snopes_urls):\n",
    "        self.urls = snopes_urls\n",
    "    \n",
    "    \n",
    "    def runSpark():\n",
    "        getQueries(self.urls)\n",
    "    \n",
    "    \n",
    "    def getQueries(urls):\n",
    "        # spark processing on urls to get keywords\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# process_urls = SparkProcessing(snopes_urls)\n",
    "# queries = process_urls.runSpark()\n",
    "\n",
    "# search = GnipData(100, 'Boulder', blah, blah)\n",
    "# search.fetchTweets(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetchTweets(query, maxResults=500):\n",
    "    # search API url\n",
    "    url = \"https://gnip-api.twitter.com/search/\" \\\n",
    "    \"fullarchive/accounts/greg-students/prod.json\"\n",
    "    \n",
    "    # storing auth credentials in a hidden variable\n",
    "    _cred=(api_user, api_pass)\n",
    "    \n",
    "    # setting parameters to append to the url\n",
    "    params = {'query':query,\n",
    "              'fromDate':'201401010000',\n",
    "              'toDate': '201703200000',\n",
    "              'maxResults': 500}\n",
    "    \n",
    "    # making a GET request to the API\n",
    "    response = requests.get(url,\n",
    "                            params=params,\n",
    "                            auth=_cred)\n",
    "    \n",
    "    # storing the number of tweets returned\n",
    "    reslen = len(response.json()['results'])\n",
    "    \n",
    "    # checking if more tweets are available\n",
    "    while ('next' in response.json().keys()):\n",
    "\n",
    "        yield reslen\n",
    "        \n",
    "        params['next'] = response.json()['next']\n",
    "\n",
    "        response = requests.get(url,\n",
    "                                params=params,\n",
    "                                auth=_cred)\n",
    "\n",
    "        reslen += len(response.json()['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using search API word locality filter\n",
    "for num_tweets in fetchTweets(\"obama thank you\"):\n",
    "    print(num_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# import configparser\n",
    "# from pykafka import KafkaClient\n",
    "\n",
    "##parameters for the search\n",
    "url =\"https://gnip-api.twitter.com/search/fullarchive/accounts/greg-students/prod.json\"\n",
    "params = { 'query':'hillary', 'maxResults':'10',\n",
    "'fromDate':'201701010000', 'toDate':'201701050000'}\n",
    "\n",
    "_cred=(api_user, api_pass)\n",
    "\n",
    "response = requests.get(url,\n",
    "                        params=params,\n",
    "                        auth=_cred)\n",
    "\n",
    "r = response.json()['results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = response.json()['results'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r['place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-81.30812, 28.69889)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tuple(ii['geo']['coordinates']) for ii in r['user']['derived']['locations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import configparser\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "#We need better storage for all of these\n",
    "#----------------------------------------\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../gnip_auth.ini')\n",
    "api_user = config['GNIP_AUTH']['username']\n",
    "api_pass = config['GNIP_AUTH']['password']\n",
    "TOPIC_NAME= \"gnipstream\"\n",
    "HOST = \"35.163.127.184\"\n",
    "KAFKA_PORT = \"32777\"\n",
    "CASSANDRA_PORT = \"9042\"\n",
    "#----------------------------------------\n",
    "\n",
    "\n",
    "class GnipData():\n",
    "    \"\"\"\n",
    "    Gets data from Gnip and pushes it to a Kafka queue. \n",
    "    Params-\n",
    "    - maxResults (int): How many Results you want\n",
    "    - maxResultsPerPage (int): Results per page, to get further we use the next param\n",
    "    - fromDate (string- yyyymmddhhmmss) : Starting Date of all the tweets returned\n",
    "    - toDate (string -yyyymmddhhmmss): Ending Date of all the tweets returned\n",
    "    \"\"\"\n",
    "    def set_up(self):\n",
    "        #self.TOPIC_NAME= \"test4\"\n",
    "        #config = configparser.ConfigParser()\n",
    "        #config.read('credentials.ini')\n",
    "        self.url = \"https://gnip-api.twitter.com/search/\" \\\n",
    "            \"fullarchive/accounts/greg-students/prod.json\"\n",
    "        #self.api_user = config['GNIP_API']['username']\n",
    "        #self.api_passwd = config['GNIP_API']['password']\n",
    "        self.kafka_server = KAFKA_ADDRESS\n",
    "        self.kafka_producer = KafkaProducer(bootstrap_servers = [self.kafka_server])\n",
    "        #this works in jupyter but not in terminal... working around\n",
    "        # self.kafka_producer = KafkaProducer(value_serializer=lambda m: json.dumps(m).encode('unicode'),\n",
    "        #                                    bootstrap_servers=[self.kafka_server])\n",
    "\n",
    "    def __init__(self, maxResults, maxResultsPerPage):\n",
    "        self.maxResults = maxResults\n",
    "        self.maxResultsPerPage = maxResultsPerPage\n",
    "        self.set_up()\n",
    "\n",
    "\n",
    "    def __init__(self, maxResults, maxResultsPerPage, fromDate, toDate):\n",
    "        self.maxResults = maxResults\n",
    "        self.fromDate = fromDate\n",
    "        self.toDate = toDate\n",
    "        self.maxResultsPerPage = maxResultsPerPage\n",
    "        self.set_up()\n",
    "\n",
    "    def fetchTweets(self, query):\n",
    "        \"\"\"\n",
    "        Takes a query and pushes the relevant tweets to Kafka\n",
    "        Params\n",
    "        - Query: Search term for Gnip\n",
    "        \n",
    "        Returns : Nothing\n",
    "        \"\"\"\n",
    "        extended_query = query+\" has:geo place_country:us\"\n",
    "        params = {'query':extended_query, \n",
    "                  'maxResults':self.maxResultsPerPage,\n",
    "                  'fromDate' : self.fromDate,\n",
    "                  'toDate' : self.toDate\n",
    "                 }\n",
    "        response = requests.get(self.url, params=params, \\\n",
    "                         auth=(api_user, api_passwd))\n",
    "\n",
    "        for r in response.json()['results']:\n",
    "                self.queueKafka( json.dumps(r).encode('utf-8'))\n",
    "\n",
    "        #Scrolling through until next runs out or maxResults is exceeded\n",
    "        for i in range(int(self.maxResults/self.maxResultsPerPage)):\n",
    "            if 'next' in response.json().keys():\n",
    "                params= {'query':extended_query, \"next\": response.json()['next']}\n",
    "                response = requests.get(self.url, params=params, \\\n",
    "                             auth=(api_user, api_passwd))\n",
    "\n",
    "                for r in response.json()['results']:\n",
    "                        self.queueKafka( json.dumps(r).encode('utf-8'))\n",
    "                #self.queueKafka(json.dumps(response.json()['results']).encode('utf-8'))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    \n",
    "    def queueKafka(self, json_data ):\n",
    "        \"\"\"\n",
    "        Queues json to Kafka\n",
    "        Params\n",
    "        - json_data: Json data to queue in Kafka\n",
    "\n",
    "        Returns : Nothing\n",
    "        \"\"\"\n",
    "        self.kafka_producer.send(TOPIC_NAME, json_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = GnipData(11, 10 , \"201612300000\", \"201612310000\")\n",
    "    a.fetchTweets(\"hillary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "import sys,json\n",
    "from enum import Enum\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "class Month(Enum):\n",
    "    Jan='01'\n",
    "    Feb='02'\n",
    "    Mar='03'\n",
    "    Apr='04'\n",
    "    May='05'\n",
    "    Jun='06'\n",
    "    Jul='07'\n",
    "    Aug='08'\n",
    "    Sep='09'\n",
    "    Oct='10'\n",
    "    Nov='11'\n",
    "    Dec='12'\n",
    "\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession.builder\\\n",
    "              .appName(\"SparkCassandraApp\")\\\n",
    "              .config(\"spark.cassandra.connection.host\", \"172.32.13.183\")\\\n",
    "              .config(\"spark.cassandra.connection.port\", \"9042\")\\\n",
    "              .master(\"local[2]\")\\\n",
    "              .getOrCreate();\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: kafka_wordcount.py <zk> <topic>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "\n",
    "    brokers, topic = sys.argv[1:]\n",
    "    sc = SparkContext(appName=\"SparkCassandraApp\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    kvs = KafkaUtils.createStream(ssc, brokers, \"spark-streaming-consumer\", {topic: 1})\n",
    "    parsed = kvs.map(lambda x: json.loads(x[1]))\n",
    "\n",
    "    def process(rdd):\n",
    "        try:\n",
    "            # Get the singleton instance of SparkSession\n",
    "            spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "            # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "            rowRdd = rdd.map(lambda tweet: Row(id=tweet['id_str']))\n",
    "            tweetsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "            tweetsDataFrame.write\\\n",
    "                .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                .options(table=\"test_tweets\", keyspace=\"swashbucklers\")\\\n",
    "                .save(mode=\"append\")\n",
    "            \n",
    "            # Creates a temporary view using the DataFrame.\n",
    "            #tweetsDataFrame.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "            # Do word count on table using SQL and print it\n",
    "            #tweetidsDataFrame = \\\n",
    "            #    spark.sql(\"select id from test_tweets\")\n",
    "            #tweetidsDataFrame.show()\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    parsed.foreachRDD(process)\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
