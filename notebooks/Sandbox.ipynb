{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import configparser\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read credentials from .ini file\n",
    "# Should be of the following form:\n",
    "# [GNIP_API]\n",
    "# username = <>\n",
    "# password = <>\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('credentials.ini')\n",
    "api_user = config['GNIP_API']['username']\n",
    "api_pass = config['GNIP_API']['password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GNIP API search filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GnipData():\n",
    "    \n",
    "    def __init__(maxResults, geolocation, fromDate, toDate):\n",
    "        self.maxResults = maxResults\n",
    "        self.geolocation = geolocation\n",
    "        self.fromDate = fromDate\n",
    "        self.toDate = toDate\n",
    "        self.url = \"https://gnip-api.twitter.com/search/\" \\\n",
    "        \"fullarchive/accounts/greg-students/prod.json\"\n",
    "    \n",
    "    \n",
    "    def fetchTweets(queries):\n",
    "        for query in queries:\n",
    "            params = {'query':query, 'maxResults':self.maxResults}\n",
    "            response = requests.get(self.url, params=params, \\\n",
    "                             auth=(api_user, api_pass))\n",
    "\n",
    "            queueKafka(response.json()['results'])\n",
    "            \n",
    "            while 'next' in response.json().keys():\n",
    "                response = requests.get(self.url, params=params, \\\n",
    "                             auth=(api_user, api_pass))\n",
    "\n",
    "                queueKafka(response.json()['results'])\n",
    "\n",
    "    \n",
    "    def queueKafka(json_data):\n",
    "        # queue json data in kafka\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SparkProcessing():\n",
    "    \n",
    "    def __init__(snopes_urls):\n",
    "        self.urls = snopes_urls\n",
    "    \n",
    "    \n",
    "    def runSpark():\n",
    "        getQueries(self.urls)\n",
    "    \n",
    "    \n",
    "    def getQueries(urls):\n",
    "        # spark processing on urls to get keywords\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_urls = SparkProcessing(snopes_urls)\n",
    "queries = process_urls.runSpark()\n",
    "\n",
    "search = GnipData(100, 'Boulder', blah, blah)\n",
    "search.fetchTweets(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetchTweets(query, maxResults=100):\n",
    "    # search API url\n",
    "    url = \"https://gnip-api.twitter.com/search/\" \\\n",
    "    \"fullarchive/accounts/greg-students/prod.json\"\n",
    "    \n",
    "    params = {'query':query, 'maxResults':maxResults}\n",
    "    \n",
    "    r = requests.get(url, params=params, auth=(api_user, api_pass))\n",
    "    \n",
    "    return r.json()['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using search API word locality filter\n",
    "data = fetchTweets(\"hillary creator isis\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual tests below this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#     try:\n",
    "#         if res['retweeted_status'] == dict:\n",
    "#             full_text = res['retweeted_status']['extended_tweet']['full_text']\n",
    "#             if \"lie\" in full_text:\n",
    "#                 print(full_text)\n",
    "#     except KeyError:\n",
    "#         print(res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#     try:\n",
    "#         url = res['retweeted_status']['entities']['urls']\n",
    "#         if (len(url) > 0):\n",
    "#             if \"\" in url[0]['unwound']['url']:\n",
    "#                 print(url[0]['unwound']['url'])\n",
    "#         if len(res['entities']['urls']) > 0:\n",
    "#             if \"fact\" in res['entities']['urls']:\n",
    "#                 print(res['entities']['urls'], \"\\n\")\n",
    "#     except KeyError:\n",
    "#         continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
