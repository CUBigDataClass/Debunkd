{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import configparser\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read credentials from .ini file\n",
    "# Should be of the following form:\n",
    "# [GNIP_API]\n",
    "# username = <>\n",
    "# password = <>\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('credentials.ini')\n",
    "api_user = config['GNIP_API']['username']\n",
    "api_pass = config['GNIP_API']['password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GNIP API search filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GnipData():\n",
    "    \n",
    "    def __init__(maxResults, geolocation, fromDate, toDate):\n",
    "        self.maxResults = maxResults\n",
    "        self.geolocation = geolocation\n",
    "        self.fromDate = fromDate\n",
    "        self.toDate = toDate\n",
    "        self.url = \"https://gnip-api.twitter.com/search/\" \\\n",
    "        \"fullarchive/accounts/greg-students/prod.json\"\n",
    "    \n",
    "    \n",
    "    def fetchTweets(queries):\n",
    "        for query in queries:\n",
    "            params = {'query':query, 'maxResults':self.maxResults}\n",
    "            response = requests.get(self.url, params=params, \\\n",
    "                             auth=(api_user, api_pass))\n",
    "\n",
    "            queueKafka(response.json()['results'])\n",
    "            \n",
    "            while 'next' in response.json().keys():\n",
    "                response = requests.get(self.url, params=params, \\\n",
    "                             auth=(api_user, api_pass))\n",
    "\n",
    "                queueKafka(response.json()['results'])\n",
    "\n",
    "    \n",
    "    def queueKafka(json_data):\n",
    "        # queue json data in kafka\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SparkProcessing():\n",
    "    \n",
    "    def __init__(snopes_urls):\n",
    "        self.urls = snopes_urls\n",
    "    \n",
    "    \n",
    "    def runSpark():\n",
    "        getQueries(self.urls)\n",
    "    \n",
    "    \n",
    "    def getQueries(urls):\n",
    "        # spark processing on urls to get keywords\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_urls = SparkProcessing(snopes_urls)\n",
    "queries = process_urls.runSpark()\n",
    "\n",
    "search = GnipData(100, 'Boulder', blah, blah)\n",
    "search.fetchTweets(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetchTweets(query, maxResults=500):\n",
    "    # search API url\n",
    "    url = \"https://gnip-api.twitter.com/search/\" \\\n",
    "    \"fullarchive/accounts/greg-students/prod.json\"\n",
    "    \n",
    "    tempArray = []\n",
    "    \n",
    "    params = {'query':query, \\\n",
    "              'fromDate':'201401010000', 'toDate': '201703200000'}\n",
    "    \n",
    "    response = requests.get(url, params=params, \\\n",
    "                            auth=(api_user, api_pass))\n",
    "    \n",
    "    temp = len(response.json()['results'])\n",
    "    \n",
    "    while 'next' in response.json().keys() and \\\n",
    "    temp < maxResults:\n",
    "                print(response.json()['next'])\n",
    "                print(temp)\n",
    "                params['next'] = response.json()['next']\n",
    "        \n",
    "                response = requests.get(url, params=params, \\\n",
    "                             auth=(api_user, api_pass))\n",
    "            \n",
    "                temp += len(response.json()['results'])\n",
    "    \n",
    "    print (temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhdXRoZW50aWNpdHkiOiIzNDAyMjNiMDc3MDE4ZTY1MmY3YjNiOGU1YmMzZDgxODI2MTYwN2RjNTA1NTFlZTI2NDAzYWI4OWY0YWEzNWQ3IiwiZnJvbURhdGUiOiIyMDE0MDEwMTAwMDAiLCJ0b0RhdGUiOiIyMDE3MDMyMDAwMDAiLCJuZXh0IjoiMjAxNzAzMTkyMDUwNTEtODQzNTY1NDU0Njg5ODgyMTExLTAifQ==\n",
      "100\n",
      "eyJhdXRoZW50aWNpdHkiOiI3OWI3OGM3NDUxM2IyZDlmMmI1YTczNzk2MjI5NzI2MDQ1MTVhYmE2NGNkZjlhZGM4MzQ2NzFlZWY3NWIyODMwIiwiZnJvbURhdGUiOiIyMDE0MDEwMTAwMDAiLCJ0b0RhdGUiOiIyMDE3MDMyMDAwMDAiLCJuZXh0IjoiMjAxNzAzMTkxNzI2NTYtODQzNTE0MTM3NTAyMjY5NDQwLTAifQ==\n",
      "200\n",
      "eyJhdXRoZW50aWNpdHkiOiI5N2I5ZGUxYmUyNDAwZTk1NmU4YWY0YWZkMDllYmMzMThmOGRmMTQ1NTEyOTI2NTNlYmI4YzJjN2E1NWUxODcxIiwiZnJvbURhdGUiOiIyMDE0MDEwMTAwMDAiLCJ0b0RhdGUiOiIyMDE3MDMyMDAwMDAiLCJuZXh0IjoiMjAxNzAzMTkxNDMyNTgtODQzNDcwMzU0MTY5MzY4NTc1LTAifQ==\n",
      "300\n",
      "eyJhdXRoZW50aWNpdHkiOiJjZGZkNWUyOTU0N2E2ZmZhNGEzYjcwOGYyMWI4YjUzODM2ODE2MzQ1ZTA1N2MzNTMzMGQwYThlNTgwMzdkMDI5IiwiZnJvbURhdGUiOiIyMDE0MDEwMTAwMDAiLCJ0b0RhdGUiOiIyMDE3MDMyMDAwMDAiLCJuZXh0IjoiMjAxNzAzMTkwOTE3NDktODQzMzkxMDQ4MTk3OTgwMTU5LTAifQ==\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# using search API word locality filter\n",
    "fetchTweets(\"obama thank you\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual tests below this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#     try:\n",
    "#         if res['retweeted_status'] == dict:\n",
    "#             full_text = res['retweeted_status']['extended_tweet']['full_text']\n",
    "#             if \"lie\" in full_text:\n",
    "#                 print(full_text)\n",
    "#     except KeyError:\n",
    "#         print(res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#     try:\n",
    "#         url = res['retweeted_status']['entities']['urls']\n",
    "#         if (len(url) > 0):\n",
    "#             if \"\" in url[0]['unwound']['url']:\n",
    "#                 print(url[0]['unwound']['url'])\n",
    "#         if len(res['entities']['urls']) > 0:\n",
    "#             if \"fact\" in res['entities']['urls']:\n",
    "#                 print(res['entities']['urls'], \"\\n\")\n",
    "#     except KeyError:\n",
    "#         continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
